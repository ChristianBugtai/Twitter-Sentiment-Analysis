{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "utils.ipynb",
      "provenance": [],
      "mount_file_id": "1AldayDE7jYicOQ3ayu5tAWOGTE3-SjWs",
      "authorship_tag": "ABX9TyOFnpWcSTxKv0JQ3EMOWFiq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChristianBugtai/Twitter-Sentiment-Analysis/blob/main/utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW4hrP7XK6eO",
        "outputId": "e76f29d3-58c5-4787-8fce-d842b8a994df"
      },
      "source": [
        "#!pip install twython"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting twython\n",
            "  Downloading https://files.pythonhosted.org/packages/24/80/579b96dfaa9b536efde883d4f0df7ea2598a6f3117a6dd572787f4a2bcfb/twython-3.8.2-py3-none-any.whl\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from twython) (1.3.0)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from twython) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.4.0->twython) (3.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->twython) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->twython) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->twython) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->twython) (2020.11.8)\n",
            "Installing collected packages: twython\n",
            "Successfully installed twython-3.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hqaGRAqJ0HZ",
        "outputId": "fbd48bc7-bf67-449c-d28d-52c70caf928f"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from twython import Twython\n",
        "import json\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import pickle\n",
        "\n",
        "\n",
        "def searchTweets(query, result_type='popular', count=1000, lang='en'):\n",
        "    \"\"\" returns a dict\"\"\"\n",
        "    \n",
        "    # Load credentials from json file\n",
        "    with open(\"/content/drive/MyDrive/Lighthouselabs/Project_Planning/Final_Project/twitter_credentials.json\", \"r\") as file:\n",
        "        creds = json.load(file)\n",
        "\n",
        "    # Instantiate an object\n",
        "    python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])\n",
        "\n",
        "    # Create our query\n",
        "    query = {'q': query,\n",
        "            'result_type': result_type,\n",
        "            'count': count,\n",
        "            'lang': lang,\n",
        "            }\n",
        "    \n",
        "    \n",
        "    # Search tweets\n",
        "    dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': [], 'location':[]}\n",
        "    for status in python_tweets.search(**query)['statuses']:\n",
        "        dict_['user'].append(status['user']['screen_name'])\n",
        "        dict_['date'].append(status['created_at'])\n",
        "        dict_['text'].append(status['text'])\n",
        "        dict_['favorite_count'].append(status['favorite_count'])\n",
        "        dict_['location'].append(status['user']['location'])\n",
        "\n",
        "    return dict_\n",
        "\n",
        "\n",
        "# Defining dictionary containing all emojis with their meanings.\n",
        "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
        "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
        "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
        "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
        "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
        "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
        "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
        "\n",
        "## Defining set containing all stopwords in english.\n",
        "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
        "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
        "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
        "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n",
        "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
        "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
        "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
        "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
        "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
        "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
        "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
        "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n",
        "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
        "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
        "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
        "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']\n",
        "\n",
        "def preprocess(textdata):\n",
        "    processedText = []\n",
        "    \n",
        "    # Create Lemmatizer and Stemmer.\n",
        "    wordLemm = WordNetLemmatizer()\n",
        "    \n",
        "    # Defining regex patterns.\n",
        "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
        "    userPattern       = '@[^\\s]+'\n",
        "    alphaPattern      = \"[^a-zA-Z0-9]\"\n",
        "    sequencePattern   = r\"(.)\\1\\1+\"\n",
        "    seqReplacePattern = r\"\\1\\1\"\n",
        "    \n",
        "    for tweet in textdata:\n",
        "        tweet = tweet.lower()\n",
        "        \n",
        "        # Replace all URls with 'URL'\n",
        "        tweet = re.sub(urlPattern,' URL',tweet)\n",
        "        # Replace all emojis.\n",
        "        for emoji in emojis.keys():\n",
        "            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])        \n",
        "        # Replace @USERNAME to 'USER'.\n",
        "        tweet = re.sub(userPattern,' USER', tweet)        \n",
        "        # Replace all non alphabets.\n",
        "        tweet = re.sub(alphaPattern, \" \", tweet)\n",
        "        # Replace 3 or more consecutive letters by 2 letter.\n",
        "        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
        "\n",
        "        tweetwords = ''\n",
        "        for word in tweet.split():\n",
        "            # Checking if the word is a stopword.\n",
        "            #if word not in stopwordlist:\n",
        "            if len(word)>1:\n",
        "                # Lemmatizing the word.\n",
        "                word = wordLemm.lemmatize(word)\n",
        "                tweetwords += (word+' ')\n",
        "            \n",
        "        processedText.append(tweetwords)\n",
        "        \n",
        "    return processedText\n",
        "\n",
        "\n",
        "def load_models():\n",
        "    '''\n",
        "    Replace '..path/' by the path of the saved models.\n",
        "    '''\n",
        "    \n",
        "    # Load the vectoriser.\n",
        "    file = open('/content/drive/MyDrive/Lighthouselabs/Project_Planning/Final_Project/vectoriser.pickle', 'rb')\n",
        "    vectoriser = pickle.load(file)\n",
        "    file.close()\n",
        "\n",
        "    # Load the LR Model.\n",
        "    file = open('/content/drive/MyDrive/Lighthouselabs/Project_Planning/Final_Project/LogisticRegression.pickle', 'rb')\n",
        "    LRmodel = pickle.load(file)\n",
        "    file.close()\n",
        "\n",
        "    ## Load the BNB Model.\n",
        "    #file = open('/content/drive/MyDrive/Lighthouselabs/Project_Planning/Final_Project/NaiveBayes.pickle', 'rb')\n",
        "    #BNBModel = pickle.load(file)\n",
        "    #file.close()\n",
        "    \n",
        "    return vectoriser, LRmodel #, BNBModel\n",
        "\n",
        "def getConfidence(sentiment, probaScore):\n",
        "    data = []\n",
        "    for i in range(len(sentiment)):\n",
        "      data.append(round(probaScore[i][sentiment[i]]*100,2))\n",
        "    return data\n",
        "\n",
        "def predict(vectoriser, model, text):\n",
        "    # Predict the sentiment\n",
        "    textdata = vectoriser.transform(preprocess(text))\n",
        "    sentiment = model.predict(textdata)\n",
        "    probaScore = LRmodel.predict_proba(textdata)\n",
        "    confidence = getConfidence(sentiment, probaScore)\n",
        "\n",
        "    \n",
        "    # Make a list of text with sentiment.\n",
        "    data = []\n",
        "    for text, pred, conf in zip(text, sentiment, confidence):\n",
        "        data.append((text,pred, conf))\n",
        "        \n",
        "    # Convert the list into a Pandas DataFrame.\n",
        "    df = pd.DataFrame(data, columns = ['text','sentiment', 'confidence'])\n",
        "    df = df.replace([0,1], [\"Negative\",\"Positive\"])\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QOzpQwzMNoz"
      },
      "source": [
        "def getSentiment(query, result_type='popular', count=1000, lang='en'):\n",
        "\n",
        "  try:\n",
        "    #get tweets\n",
        "    tweets = searchTweets(query=query, result_type=result_type, count=count, lang=lang)\n",
        "    #import model\n",
        "    vectoriser, LRmodel = load_models()\n",
        "    #predict\n",
        "    df = predict(vectoriser, LRmodel, tweets['text'])\n",
        "\n",
        "    return df\n",
        "  except ValueError:\n",
        "    print('No Results')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz9TZQEa5pKl"
      },
      "source": [
        "vectoriser, LRmodel = load_models()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu5wQnfd5Iq1"
      },
      "source": [
        "TEXT = ['This text is just a test, i love DataScience!']\n",
        "pred = predict(vectoriser, LRmodel, TEXT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "a-jxMLfV5iyu",
        "outputId": "a64e7885-c95d-4bdd-d00f-8e29c0a89139"
      },
      "source": [
        "pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This text is just a test, i love DataScience!</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            text sentiment\n",
              "0  This text is just a test, i love DataScience!  Positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB2ylElc5ySO"
      },
      "source": [
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayLw-nR_3cT7",
        "outputId": "4c2aa3aa-11b5-4224-a81d-8de826314310"
      },
      "source": [
        "query = 'covid'\n",
        "parameters = {'q':f'{query}','lang':'en','count':100}\n",
        "r = requests.get('https://api.twitter.com/1.1/search/tweets.json?', params=parameters)\n",
        "print(r.url)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://api.twitter.com/1.1/search/tweets.json?q=covid&lang=en&count=100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3snGUsE3ieA",
        "outputId": "d9d00343-fe71-4b8e-f370-eeb84a3ccd7f"
      },
      "source": [
        "!pip install twitter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting twitter\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/e2/f602e3f584503f03e0389491b251464f8ecfe2596ac86e6b9068fe7419d3/twitter-1.18.0-py2.py3-none-any.whl (54kB)\n",
            "\r\u001b[K     |██████                          | 10kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20kB 20.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 40kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.1MB/s \n",
            "\u001b[?25hInstalling collected packages: twitter\n",
            "Successfully installed twitter-1.18.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXQm5Cs56Rf-"
      },
      "source": [
        "from twitter import *\n",
        "import json\n",
        "from IPython.display import JSON"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oeR4t1d6WQX"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/Lighthouselabs/Project_Planning/Final_Project/twitter_credentials.json\", \"r\") as file:\n",
        "        creds = json.load(file)\n",
        "t = Twitter(\n",
        "    auth=OAuth(creds['ACCESS_TOKEN'], creds['ACCESS_SECRET'], creds['CONSUMER_KEY'], creds['CONSUMER_SECRET']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE4HDewG67Et"
      },
      "source": [
        "result = t.search.tweets(q=\"data science\", lang = 'en', count = 100, result_type = 'mixed')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDgGnQQQENoT",
        "outputId": "f5930fe5-59c2-4227-d882-649aff1a72e1"
      },
      "source": [
        "len(result['statuses'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w4W4tla85qO"
      },
      "source": [
        "count = result['search_metadata']['count']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3SCA0VoA7YM"
      },
      "source": [
        "# Search tweets\n",
        "dict_ = {'user': [], 'date': [], 'text': [], 'location':[]}\n",
        "for i in range(count):\n",
        "  dict_['user'].append(result['statuses'][i]['user']['screen_name'])\n",
        "  dict_['date'].append(result['statuses'][i]['created_at'])\n",
        "  dict_['text'].append(result['statuses'][i]['text'])\n",
        "  dict_['location'].append(result['statuses'][i]['user']['location'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "67H22lbdCWSJ",
        "outputId": "7b01d8c3-bd00-4404-8c8f-0bacc4f1f5c6"
      },
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(dict_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>location</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Jillie_Alexis</td>\n",
              "      <td>Sun Nov 29 18:30:22 +0000 2020</td>\n",
              "      <td>There is no data or science that supports the ...</td>\n",
              "      <td>Washington, DC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BogochIsaac</td>\n",
              "      <td>Mon Nov 30 12:14:32 +0000 2020</td>\n",
              "      <td>Moderna's #COVID19 vaccine final results are i...</td>\n",
              "      <td>University of Toronto</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SecAzar</td>\n",
              "      <td>Mon Nov 30 13:40:09 +0000 2020</td>\n",
              "      <td>With today's announcement from Moderna, we'll ...</td>\n",
              "      <td>Washington, DC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sdimaso</td>\n",
              "      <td>Mon Nov 30 13:43:33 +0000 2020</td>\n",
              "      <td>Why? Our RT was 1.14 as of yesterday! \\nGov he...</td>\n",
              "      <td>New Jersey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>laura_nelson</td>\n",
              "      <td>Mon Nov 30 00:08:52 +0000 2020</td>\n",
              "      <td>There are about 15 people here in Echo Park pr...</td>\n",
              "      <td>Los Angeles, CA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>johnandkenshow</td>\n",
              "      <td>Mon Nov 30 22:10:54 +0000 2020</td>\n",
              "      <td>No Science - No Data - No Shutdown // Dozens o...</td>\n",
              "      <td>Southern California</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>fchollet</td>\n",
              "      <td>Mon Nov 30 16:51:03 +0000 2020</td>\n",
              "      <td>Every year, Kaggle runs a large-sample-size su...</td>\n",
              "      <td>United States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Esri</td>\n",
              "      <td>Sun Nov 29 18:25:04 +0000 2020</td>\n",
              "      <td>Find free guided lessons in imagery and remote...</td>\n",
              "      <td>Redlands, CA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>SteveHiltonx</td>\n",
              "      <td>Tue Dec 01 01:44:09 +0000 2020</td>\n",
              "      <td>why are schools STILL closed when DATA, SCIENC...</td>\n",
              "      <td>California</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>wef</td>\n",
              "      <td>Mon Nov 30 05:00:04 +0000 2020</td>\n",
              "      <td>The key to stopping Alzheimer's, according to ...</td>\n",
              "      <td>Geneva, Switzerland</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             user  ...               location\n",
              "0   Jillie_Alexis  ...         Washington, DC\n",
              "1     BogochIsaac  ...  University of Toronto\n",
              "2         SecAzar  ...         Washington, DC\n",
              "3         Sdimaso  ...             New Jersey\n",
              "4    laura_nelson  ...        Los Angeles, CA\n",
              "5  johnandkenshow  ...    Southern California\n",
              "6        fchollet  ...          United States\n",
              "7            Esri  ...           Redlands, CA\n",
              "8    SteveHiltonx  ...             California\n",
              "9             wef  ...    Geneva, Switzerland\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTaIfqxrCbz2"
      },
      "source": [
        "def searchTweets(query, count=100, result_type='popular'):\n",
        "    \"\"\" returns a dict\"\"\"\n",
        "    \n",
        "    # Load credentials from json file\n",
        "    with open(\"/content/drive/MyDrive/Lighthouselabs/Project_Planning/Final_Project/twitter_credentials.json\", \"r\") as file:\n",
        "        creds = json.load(file)\n",
        "    \n",
        "    # Instantiate an object\n",
        "    t = Twitter(\n",
        "    auth=OAuth(creds['ACCESS_TOKEN'], creds['ACCESS_SECRET'], creds['CONSUMER_KEY'], creds['CONSUMER_SECRET']))\n",
        "\n",
        "    # Create our query\n",
        "    result = t.search.tweets(q=query, count = count, result_type = result_type, lang = 'en')\n",
        "    \n",
        "    \n",
        "    # Search tweets\n",
        "    dict_ = {'user': [], 'date': [], 'text': [], 'location':[]}\n",
        "    for i in range(count):\n",
        "      try:\n",
        "        dict_['user'].append(result['statuses'][i]['user']['screen_name'])\n",
        "        dict_['date'].append(result['statuses'][i]['created_at'])\n",
        "        dict_['text'].append(result['statuses'][i]['text'])\n",
        "        dict_['location'].append(result['statuses'][i]['user']['location'])\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "    return dict_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "hI2fwUEkDhq3",
        "outputId": "4a18e5e6-5256-4b98-b19b-ff83aa6e7eb0"
      },
      "source": [
        "result = getSentiment('covid')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UFuncTypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-158-b1b36576e459>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'covid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-154-633a621bd8d2>\u001b[0m in \u001b[0;36mgetSentiment\u001b[0;34m(query, result_type, count, lang)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvectoriser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLRmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectoriser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLRmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-157-5c1c88823cb4>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(vectoriser, model, text)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mprobaScore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLRmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mconfidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetConfidence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobaScore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-157-5c1c88823cb4>\u001b[0m in \u001b[0;36mgetConfidence\u001b[0;34m(sentiment, probaScore)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m       \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobaScore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "FguBT_D9DngU",
        "outputId": "28601936-6f85-4e56-e655-709da99f06dc"
      },
      "source": [
        "result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>confidence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33,772,990 people around the world have gotten...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>52.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>My father-in-law ‘Coco’s dad’ was a serious ‘N...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>89.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Our response to covid sometimes makes me feel ...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>70.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Percent of wages currently subsidized by gover...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>52.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A friend’s husband, quarantined at home with C...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>87.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>I strongly support this Supreme Court ruling d...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>78.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>8+ months into COVID and the average American ...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>56.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>We have no time to lose to address the economi...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>63.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>To all the “tough guys\" who don't want to wear...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>59.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Florida Gov. Ron DeSantis says public schools ...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>53.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Alyssa, facts matter. Dems have TWICE filibust...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>71.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>The Senate is back. We’re in session. America ...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>74.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>RT to tell @senatemajldr that Americans need a...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>77.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>.@AliVelshi is covering the horror story of @K...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>51.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Why did PA Pass No Excuse Mail in voting befor...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>90.41</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 text sentiment  confidence\n",
              "0   33,772,990 people around the world have gotten...  Negative       52.57\n",
              "1   My father-in-law ‘Coco’s dad’ was a serious ‘N...  Negative       89.51\n",
              "2   Our response to covid sometimes makes me feel ...  Negative       70.53\n",
              "3   Percent of wages currently subsidized by gover...  Positive       52.62\n",
              "4   A friend’s husband, quarantined at home with C...  Negative       87.99\n",
              "5   I strongly support this Supreme Court ruling d...  Positive       78.24\n",
              "6   8+ months into COVID and the average American ...  Negative       56.04\n",
              "7   We have no time to lose to address the economi...  Negative       63.57\n",
              "8   To all the “tough guys\" who don't want to wear...  Negative       59.17\n",
              "9   Florida Gov. Ron DeSantis says public schools ...  Negative       53.52\n",
              "10  Alyssa, facts matter. Dems have TWICE filibust...  Positive       71.12\n",
              "11  The Senate is back. We’re in session. America ...  Positive       74.48\n",
              "12  RT to tell @senatemajldr that Americans need a...  Positive       77.42\n",
              "13  .@AliVelshi is covering the horror story of @K...  Positive       51.94\n",
              "14  Why did PA Pass No Excuse Mail in voting befor...  Negative       90.41"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGxrsM3TnKvM"
      },
      "source": [
        "vectoriser, LRmodel = load_models()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExzYYMtPDxXq"
      },
      "source": [
        "textdata = vectoriser.transform(preprocess(['I Hate Python!','I love Python!', 'python is not bad', 'python is not good']))\n",
        "sentiment = LRmodel.predict(textdata)\n",
        "probaScore = LRmodel.predict_proba(textdata)\n",
        "confidence = getConfidence(sentiment, probaScore)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4F1xeEFnc3g",
        "outputId": "4c5bae7e-3d9c-4384-f198-ba8ec7c60591"
      },
      "source": [
        "probaScore"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.96285099, 0.03714901],\n",
              "       [0.0203129 , 0.9796871 ],\n",
              "       [0.22348136, 0.77651864],\n",
              "       [0.98662315, 0.01337685]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQlaaUe0ngcl",
        "outputId": "20f91adc-1f68-4add-98f9-72d29f17a5d2"
      },
      "source": [
        "sentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWdJz1lFobnx",
        "outputId": "3b3f93e0-6f9c-4cee-c804-57313a88bd13"
      },
      "source": [
        "confidence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97.96870986282545"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYXH-kjCrTKQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoUvj3CBtBgl",
        "outputId": "c7910a2a-c04d-429b-f147-d6a6d03deb50"
      },
      "source": [
        "getConfidence(sentiment, probaScore)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[96.29, 97.97, 77.65, 98.66]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhFafgW7tHJT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}